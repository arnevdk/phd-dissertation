\documentclass{letter}
\usepackage[backend=biber]{biblatex}

\addbibresource{references.bib}

\newcommand{\reply}[1]{%
	\textbf{Response to review comments made by prof. #1}

}

\begin{document}

\signature{%
	Arne Van Den Kerchove \\
	\texttt{arne.vandenkerchove@kuleuven.be} \\
	+32 473 32 78 71 \\
}
\address{%
	KU Leuven \\
	Laboratory for Neuro- and Psychophysiology \\
	Campus Gasthuisberg, O\&N 2 \\
	Herestraat 49 bus 1021 \\
	BE-3000 Leuven
}

\begin{letter}{%
	prof. John Creemers \\
	KU Leuven \\
	Doctoral School of Biomedical Sciences \\
	Campus Gasthuisberg, O\&N 4 \\
	Herestraat 49 bus 700 \\
	BE-3000 Leuven

}
\opening{Dear chair, secretary and members of the reading committee}

I would like to extend my sincere gratitude for the time and effort you have taken
to review the manuscript of my PhD dissertation.
Additionally, I would like to thank you for the insightful, constructive and
detailed comments you all have provided.
The following reply contains a point-by-point response to your review comments.
Attached, you will also find an updated draft of the manuscript, as well as a
version with indicated changes for your approval.

\reply{Reinhold Scherer}
Major comments:
\begin{enumerate}
	\item Throughout the text, qualitative statements have been quantified where possible and appropriate.
	\item  All mathematical equations and derivations have thoroughly
	been checked for correctness and notational consistency, and have been
	corrected where necessary.
	\item As indicated by the reviewer, true on-line experiments with live
	feedback have indeed not been reported on in this thesis,
  but were foreseen to take	place within the framework of this doctoral study.
	I agree that these experiments would be valuable to supplement the
	presented conclusions and are the logical next step.
	Indeed, the start of experimentation with healthy control participants,
	as well as with patient groups, was heavily delayed by the COVID-19 pandemic, as the
	preparatory research started in September 2019 and I officially
	enrolled in the doctoral training program in January 2020.
	This restricted not only our access to experiment subjects for a
	significant period at the start of the PhD -- and even longer for
	vulnerable patient groups -- but also shifted priorities for the medical
	staff at our partner institutions and patient centers.
	Hence, work early in the PhD mainly focused on the development of
	computational methods.
	We are currently starting up true on-line experimentation in cooperation
	with our partner patient centers, with an adapted interface and experimental protocol.
  This limitation has now clearly been mentioned in section 8.2.
\end{enumerate}
Minor comments:
\begin{enumerate}
	\item Recorded datasets could not be made publicly available since the
	ethical protocol (Ethics Commission of University Hospital Leuven,
	S62547 v1.4) for the conducted studies did not include provisions
	publishing the data.
\end{enumerate}

\reply{Fabien Lotte}
Major comments:
\begin{enumerate}
	\item
    Part of the confusion between active and passive BCI might be due
	  to the poor choice of phrasing ``paradigms that require little active
    participation from the user, like redirecting attention or initiating
    imagined actions, are classified as passive BCI.''
	  This has been rewritten to ``in contrast to paradigms that require the
    user's active participation, for
	  instance through redirecting attention or initiating imagined actions,
	  paradigms free of this requirement are classified as passive BCI,''
	  for clarity.
	  Neurofeedback has been reclassified as active BCI in the text and in
	  figure 1.3.
	\item I agree that investigating the effect of shrinkage regularization on the
    performance of XDAWN+RG is necessary to obtain a fair comparison method.
    This is especially in the case of interest here, when data are scarcely
    available and the proposed methods also rely on shrinkage.
    While setting up the decoding performance comparison experiment, we
    evaluated performance with and without Ledoit-Wolf shrinkage for both the
    estimation of the XDAWN filters and the subsequent covariance matrices.
    Indeed, we expected that this regularization would increase performance,
    just as in the STBF case. However, we noticed that highest accuracy for the
    XDAWN+RG method was reached without shrinkage in the XDAWN filter
    estimation and the covariance estimation, and when retaining a sample rate
    of 2048 Hz.
    This has now clearly been indicated in the manuscript.
  \item
    As indicated by the reviewer, however, using a 2048 Hz sample rate for one of the
    classifiers, while 32 Hz has been used for the others, gives a skewed image
    of the training time.
    A fair comparison of training times has now been reported in section 1.3.3
    and figure 1.4 with XDAWN+RG also using a 32 Hz sample rate.
  \item Appendix 1.A now fully reports all obtained accuracies to give a better
    insight in effect sizes. Wherever the significant effects have been mentioned
    in the text, the effect size has now been reported along with it.
	\item
    In the results section of chapter 4, statistical comparisons were
	  only performed between methods for which we evaluated the performance
	  ourselves (HODA, PARAFACDA and BTTDA).
	  Since the MOABB benchmarking framework~\cite{Aristimunha2023} allows for
	  fair comparison of performance metrics reported across different works,
    the analyses for other comparison methods were not re-run by us (as
    reported).
    Instead, but performances were taken from the MOABB benchmark database~\cite{Chevallier2024}.
	  Unfortunately, this source provides only the aggregated performance metrics
	  and not the metrics per subject, session and cross-validation fold,
	  hence we could not perform relevant statistical comparison with these
	  comparison models.
  \item
    Tables 6.2 and 6.3 now report the absolute cross-condition ROC-AUC values
    per model and dataset for a more complete and interpretable presentation of
    the results, and the absolute values have been mentioned when referring to
    effects in the results section.
\end{enumerate}
Minor comments:
\begin{enumerate}
	\item According to the annotations, typographical errors have been
	corrected in text and figures, and imprecise or inaccurate statements have been adjusted.
	The recommended references have been discussed and, where necessary,
	statements were updated accordingly.
  Figure 1.5 was corrected and matched with the correct caption.
\end{enumerate}

\reply{Andrea K\"ubler}
Major comments:
\begin{enumerate}
  \item  I agree that the presented work could have been strengthened by the
  formulation of clearly testable hypotheses, including a general hypothesis
  and per-study subhypotheses.
  I would partially attribute the lack thereof to the exploratory and
  engineering-oriented nature of this thesis.
  The original hypotheses were rephrased as directed research objectives in
  section 2.3, but these have now again explicitly been reformulated to the
  following hypothesis: \emph{``a suited decoding strategy can improve the
    (gaze-independent) accuracy of a spatially organized visual oddball
    ERP-based brain''}.
  The findings regarding this hypothesis have now also been discussed
    explicitly in section 8.1.3.
  \item As mentioned above in the reply to prof. Reinhold Scherer's third major
    comment, this thesis does indeed not report on the foreseen on-line
    experimentation due to several factors.
    Current work focuses on on-line evaluation with user interest groups.
  \item
    A clearer distinction between methods, results and discussion has been made
    in several chapters.
    In chapter 5, the synthetic data generation procedure has been
    moved to the method section as section 5.3.4.
    In chapter 6, the analysis investigating observed jitter per condition has
    been moved out of the discussion section and has been given a place in the
    results section as section 6.3.3.
    Sections in chapter 7 have been renamed to follow a proper IMRaD structure.
    Here, the description of visual motor assessment and the
    discussion of outcomes regard visual skills have been separated and placed
    respectively in the methods and results section.
    Since chapter 4 focuses on methodological contributions which are then
    verified in experiments that are rather independent from these methods, I
    believe it is clearer to keep the presentation of the evaluation methods
    and the results together per data modality in a separate `Experiments'
    section, instead of also moving the description of evaluation methods to
    the method section.
  \item I very much appreciate the instruction of the reviewer to further
    investigate user-centered design (UCD)~\cite{Standardization2009,Kuebler2014}.
    In fact, there was a large overlap between the principles and methods laid
    out in the UCD framework and the topics discussed in section 8.2.
    The UCD standard and its applications to BCI research provide a structured
    approach to discussing the limitations and recommendations arising from this
    work.
    Furthermore, they form a guideline for conducting ongoing and future on-line patient
    studies.
    UCD has now extensively been referenced throughout chapter 8.
\end{enumerate}
Minor comments:
\begin{enumerate}
  \item The description of fMRI and fNIRS has been
    corrected to reflect the properties of the BOLD signal and the fNIRS blood
    oxygenation signal.
  \item	Neurofeedback has been reclassified as active BCI in the text and in
	  figure 1.3.
    The introduction to passive BCIs has been rephrased to focus on
    \emph{control through detection} instead of only \emph{monitoring}.
    Monitoring can be a part of passive BCIs, e.g. for clinical use, but I
    agree that the core focus should lie here indeed on how to use the output
    for useful control.
    Figure 1.3 has been updated accordingly.
  \item A brief review of the state-of-the art of tactile BCIs in the
    context of this thesis has been added to section 2.2.1.
  \item I agree that loss of vision is an important consideration which affects
    the merits of specific BCI paradigms. This has now been discussed in the
    literature review presented in section 2.2.1. The research objectives
    put forward in section 2.3 state now state clearly that our study concerns
    those individuals retaining sufficient vision.
\end{enumerate}

\reply{Maarten De Vos}
Major comments:
\begin{enumerate}
	\item Mathematical equations and derivations have thoroughly
	been checked for correctness and notational consistency, and have been
	corrected throughout the manuscript where necessary.
	The paragraph on Block-Toeplitz linear discriminant analysis (tLDA) in
	section 1.6.4 has also been corrected, and it was expanded for clarity.
	Figures 3.1 and 3.2 have been regenerated and relabeled for consistent
	formatting.
	\item The Wilcoxon signed-rank, a non-parametric,
    paired test was used to compare ROC-AUC values in sections 4.3.3 and 6.3.1.
    I agree that the DeLong test is indeed an appropriate test for
    statistically comparing ROC-AUCs between two models, but it is commonly used
    to compare ROCs obtained from a single test dataset (potentially obtained
    through stacking cross-validated predictions).
    Our experiments, on the other hand, involve calculating ROC-AUCs per
    cross-validation fold, yielding datasets of multiple ROC-AUCs to compare.
    In this case, the Wilcoxon signed-rank test is the appropriate
    test~\cite{Rainio2024}.

    While both approaches are valid strategies and commonly used~\cite{Rainio2024}, I believe
    the latter is more interpretable in the context of the generalizability
    of machine learning models, since it more directly reflects the variance
    across cross-validation folds.
    Hence, I recognize the potential application of the DeLong test here,
    but I would argue that using Wilcoxon signed-rank tests in combination with
    the evaluation strategy employed in the experiments is still a valid approach.
   \end{enumerate}
Minor comments:
\begin{enumerate}
	\item Unsupported claims regarding generalizability have been omitted
	from the abstract.
	\item Throughout the manuscript, different EEG band-pass filtering
	settings have been used.
	Chapter 1 indicates that ERPs are usually band-pass filtered between
	0.5 and 16 Hz.
	These were also the preferred cut-off frequencies for other
	(unpublished) analyses used during the course of the thesis work, an
	those used for the analyses in chapter 3.

	However, ERP datasets evaluated in chapter 4 were band-pass filtered
	between 1 and 24 Hz, and in chapter 6 between 0.1 and 20 Hz.
	While there is no general consensus in the field about the optimal
	cut-off frequencies for P300 detection and ERP analysis in general,
	these values all fall well within the accepted
	bounds~\cite{Bougrain2012}.
	Decisions to deviate from this were made consciously.

	In chapter 3, filtering between 1 and 24 Hz was applied for consistency
	with the recommended ERP preprocessing pipeline of the MOABB
	benchmarking framework~\cite{Aristimunha2023}.
	This allows for a fair comparison under equal circumstances of the
	performance of the proposed algorithm with performances of other
	algorithms reported by the MOABB benchmarking
	database~\cite{Chevallier2024}.

	In chapter 6, the low-pass frequency was increased from 16 Hz to 20 Hz,
	since the analyses in this explicitly involve the short-time effects of
	latencies of ERP components, and the analysis of faster, early ERP
	components. A low low-pass frequency would harm the precision of
	latency effects in the filtered signal.
	A lower high-pass filter of 0.1 Hz was used here to reveal the
	influence of the proposed alignment approach on potential
	slower late components.
	\item Typographical errors have been corrected throughout the
	manuscript.
\end{enumerate}

\clearpage

\reply{Adalberto Simeone}
Major comments:
\begin{enumerate}
	\item Section 8.3.2 has been expanded to discuss the limitations stemming
    from the lack of applied, real-world testing.
    We are currently addressing these issues by performing a study with an
    on-line BCI assistive technology interface with realistic selection
    options and evaluation by end users.
\end{enumerate}
Minor comments:
\begin{enumerate}
	\item Measurement unit representation has been uniformized according to the
    SI standard throughout the text and in figures 5.4-5.6.
  \item Numerical citations have now consistently been replaced by text-style citations
    mentioning the author names where appropriate in the sentence.
  \item Contractions have been removed throughout the text.
\end{enumerate}


\textbf{References}

\printbibliography[heading=none]

\closing{Kind regards and thank you in advance,}
\end{letter}
\end{document}
