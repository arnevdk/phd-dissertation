\todo{\emph{Goal:} Enable communication for eye-motor impaired patients}

\todo{Check provisionary and final doctoral plan for motivation}
\todo{\emph{Method:} Design a comfortable interface that allows them to maximally exploit
their residual gaze capabilities, by leveraging a non-invasive high-ITR
visuospatial ERP-based BCI and improving ERP decoding performance (in general
and specifically in gaze-independent settings).}
\todo{check WIP seminar}

\todo{update text}

\emph{Novelty:}

Brain-Computer Interfaces (BCIs) decode brain activity with the aim to establish a direct communication
pathway bypassing speech and other forms of muscular activity. They have raised great hopes for patients
devoid of these abilities, for whom BCIs can provide a means to communicate or to
control devices.
The quest for performant and affordable solutions is most evident in
visual BCIs based on the electroencephalogram (EEG), where it has led to a gamut of increasingly sophisticated decoders
and paradigms tailored to the needs of specific stimulation paradigms and
use contexts.
An effective and proven method is the visual event-related potential (ERP)-based
oddball interface.
Here, targets are shown in short pulses on a computer screen.
Each time the user observes a target, an ERP is evoked, the presence of which can be
detected in the EEG-signal.
The ERP consists of multiple components, of which some are modulated by the
attention of the user, specifically the N2 and the P3 component.
Decoding this modulation allows for information transfer controlled by the
user's brain activity, as sketched in Figure~\ref{fig:bci}.
However, visual BCIs cannot operate efficiently when the user does not direct
their gaze onto the
desired target~\cite{Brunner2010}.

This leads to an often unadressed paradox: the BCI target population consists
of patients who suffer from pathologies like Amyotrophic Lateral Sclerosis
(ALS), Multiple Sclerosis (MS), stroke (specifically
brain-stem stroke) or traumatic brain injury, which can lead to several degrees
of paralysis, or even to Locked-in Syndrome (LiS), the complete loss of muscle
control with preserved consciousness.
While BCIs are most attractive as a solution for these patients
compared to other assistive technologies like eye-tracking, studies often
report bad performance in patient populations precisely due to the lack of adequate eye motor
control, or patients with gaze impairments are excluded.
Table~\ref{tab:incidence} reports the relatively high frequencies of
eye motor problems, which can range from involuntary eye movements to control
issues and even to partial or complete ophtalmoplegia.
\begin{table}
	\centering
	\begin{tabular}{@{}|l|rrrrr|r|@{}}
		\hline
		         & ALS  & MS   & Stroke  &
		DMD      & SMA  & LiS                           \\ \hline
		Minor    & 50\% & 31\% & 40-70\% & + & - &      \\
		Severe   & 33\% & 3\%  & +       & - & - & 98\% \\
		Complete & 17\% & -    & +       & - & - & 2\%  \\
		\hline
	\end{tabular}
	\caption{Incidence of eye motor impairment in Brain-Computer Interface target
		populations: (ALS: Amyotrophic Lateral Sclerosis, MS: Multiple Sclerosis,
		DMD: Duchenne's Muscular Dystrophy, SMA: Spinal Muscular Atrophy, LiS:
		Locked-In-Syndrome).}\label{tab:incidence}
\end{table}
\todo{document methodology}
\todo{find more references, fill out}
Different degrees of eye motor impairment can render visual BCIs
uncomfortable or outright impossible to use.
When operating a visual BCI, the usual rapid series of forced saccades followed
by fixation is tiring over time, even for healthy control subjects.
A suitable alternative would allow the user to keep their eyes in an at-rest
position of their choice while operating the BCI.
This points to the need for gaze-independent BCIs.


\section{Objective}

The general goal of this PhD is to tackle the gaze-dependency in visual BCIs.
Traditionally, gaze-independency~\cite{Riccio2012, Aloise2012} can be realized in three ways:
Firstly, by avoiding visual
stimulation entirely and opting for auditory or somatosensory stimulation instead.
However, these alternatives often result in lower information transfer rates,
increased mental effort and variable outcomes for different users.
Secondly, gaze-independence can
be achieved by adapting the interface to display stimuli such that they
are always in the center of the field of view, by exploiting visual
non-spatial attention (feature attention), or a combination thereof.
These interfaces suffer equally from reduced information transfer rates.
Moreover, they still rely to some extent on eye motor control, necessitating central gaze fixation.
We make a distinction here between spatially organized interfaces, where
multiple targets are displayed at the same time at different spatial locations,
and temporally organized interfaces, like Rapid Serial Visual Presentation,
where targets or small sets of targets are shown consecutively.
Traditionally, spatially organized interfaces have a higher information transfer
rate, but are also more gaze-dependent than temporally organized interfaces.
Third, stimuli can be presented in a standard BCI paradigm, but \emph{mental} attention
can be decoded separately from \emph{visual} attention.
\todo{table with paradigms and itrs from WIP seminar}

This leads us to adopt a specific approach: improve the performance of a
spatially organized visual oddball ERP-based brain
computer interface by using a suited decoding strategy.
Hence, our working hypothesis is as follows:
\begin{quote}
	\textit{A spatially organized visual oddball ERP BCI with a suited decoder
		can achieve similar or higher information transfer rate in patient
		populations with eye motor deficits than other
		gaze-independent alternatives described in literature.}
\end{quote}

To achieve our goal, we innovate on decoder development and interface design,
and we collect data from healthy control subjects, which allow us to test our decoders.
Finally, the findings of these experiments will be leveraged to develop a BCI
for patient use, which will be tested in a case-based patient study in which
both performance and user comfort will be evaluated.


\section{Methods}
\todo{update with new decoders and experiments}

\subsection{Interface design}
We designed a visual oddball interface that helps us study and adapt
to the effects of eye motor impairment on the ERP, when using existing state-of-the-art
decoders and on our own proposed decoders.
In a first series of experiments, we recorded data with this interface from healthy participants.
The goal of these experiments is to benchmark
gaze-independent ERP decoding algorithms.
We denote this dataset as the Covert Visuospatial Attention ERP dataset
(CVSA-ERP)
This study was approved by the Ethics Commission of University Hospital Leuven
(S62547).

Using a hexagonal lay-out interface, similar to the visual Hex-o-Spell proposed
by Treder \& Blankertz~\cite{Treder2010}, we present six flashing targets (without letters or
symbols) to the participant while the EEG and electro-oculogram (EOG) are
recorded, as well as eye gaze using eye tracking.
To simulate the dissociation between the eye gaze (visual attention) and the
intended target (mental attention) which could occur in patients with hampered eye
motor control, healthy participants are cued to operate the BCI in specific
visuospatial attention (VSA) conditions.
Each subject performs 5 different VSA conditions as illustrated in
Figure~\ref{fig:vsa-conditions}.
\begin{figure}
	%\includegraphics[width=\linewidth]{figures/attention_modes.eps}
	\caption{Visuospatial attention (VSA) conditions in our experiment. The user is
		asked to dissociate their visual and mental attention by gazing at a
		crosshair while attending a target, depending on the prompted condition.}%
	\label{fig:vsa-conditions}
\end{figure}
\todo{new picture that shows more clearly a user operating freely where they
can gaze}
In the overt case, subjects are instructed to gaze at the cued
target they are also mentally attending; in the covert case, subjects are instructed
to gaze at the center of the screen while mentally attending to the cued target.
Finally, we introduce a VSA condition that is understudied in the context of
gaze-independent BCI development: split VSA.
In split VSA, the participant mentally focuses on one cued target while gazing at
another (the distractor).
Depending on the inter-target distance along the hull of the hexagon between the
attended target and the distractor we discern three split VSA subconditions:
the distractor is either clockwise or counterclockwise directly next to the
attended target ($d=1$), there is one other target between the attended target and
the distractor ($d=2$), or the distractor is opposite the intended target
($d=3$).


After data collection from healthy participants and decoder development, we
will carry out a patient study using a similar interface.
The goal of the patient study is to evaluate the influence of eye motor
deficits in patient populations on existing state-of-the-art decoders and on
our proposed decoding methods.
\todo{update with current results}

\subsection{Decoders}

During this PhD, we explored different lines in decoding strategies,
trying to tackle several problems that arise from gaze-independence, such as
the lack or decrease in amplitude of specific ERP components, and the increased
non-stationarity of the signal.
As mentioned, state-of-the-art decoders have poor performance in covert attention
settings.
The general goal is thus to design a machine learning classifier that represents
the ERP signal in a specific way, such that it becomes more robust to the problems
occurring in covert attention conditions.

\subsubsection{Regularized spatiotemporal beamforming}
Due to the lack of the N2 component and decrease of amplitude of the
P3 component in covert attention
settings~\cite{Treder2010}, the signal-to-noise ratio (SNR) of
the ERP is lower than in overt attention settings.
Therefore, a straightforward way to reach satisfactory
gaze-independent decoding performance, might be by increasing overall ERP
decoding performance. A more accurate classifier could yield relative
improvements in those settings where performance is not yet near the achievable
maximum.

Therefore, we have improved upon an in-house developed, state-of-the art ERP
decoder, the spatiotemporal beamformer~\cite{Wittevrongel2016}, by reformulating
this classifier as a linear discrimination problem and
imposing regularizing constraints by structuring the noise covariance matrix
(STBF-struct).
Furthermore, these regularizing constraints impose temporal stationarity on
the background noise, which is of use in our next efforts to cope with the
non-stationarity of the P3 signal component.

\subsubsection{Classifier-based Latency Estimation with Woody iterations}
Literature shows that better covert attention performance can be achieved by
counteracting the non-stationarity of the P3 ERP component in covert
attention settings~\cite{Arico2014}.
This can be done by estimating the latency of each single-trial ERP and aligning
the P3 peaks to all fall at the same moment.
The resulting aligned data and the set of latencies can then be used to train a
classifier that is more robust to jitter.

Existing latency estimation methods are either not applicable to the
classification problem of labeling unseen data, or are not robust enough to
deal with the low SNR of the ERP.
Classifier-based Latency Estimation (CBLE)~\cite{Mowla2017} is a technique that can leverage
ERP latency estimation in a decoding setting, but our results show that it
yields no improvement in gaze-independent settings.
We improved upon this technique and extended it to a probabilistic, iterative
method named Classifier-based Latency Estimation with Woody iterations (wCBLE).

\todo{update with current results and all chapters}
