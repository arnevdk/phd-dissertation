\input{chapters/bttda/tensorviz.tex}
\todo{abbreviations}

\section{Introduction}
\todo{immediately arrive to the novelty or to the point}
\todo{reduce length of the introduction}
\todo{less acronyms}
\todo{put the intuition behind the algorithm (sparse, more flexible, what it
does in one sentence ) in the introduction. Additionally "try to get some
information from the data, continue extracting more information as long as
there is some left}
Brain-computer interfaces (BCIs) have the potential to bypass
defective neural pathways by providing an alternative communication channel
between the brain and an external device.
These interfaces find applications in the development of neuroprosthetics and assistive
technologies, among other applications~\cite{Wolpaw2020}.
To achieve their functionality, BCIs record and process neural data obtained through
a neuroimaging technique, with electroencephalography (EEG) being the most popular.
\todo{explicitly state how BTD is a generalization of PARAFAC and TUCKER}

Due to its multichannel time series structure, EEG data, like most neural
signal acquisition modalities used for BCIs, naturally exist as multiway data,
capturing information in both spatial and temporal domains.
Common preprocessing transformations, such as time-frequency transformation,
time-binning, or integrating information across multiple subjects or conditions,
can further expand the data into additional analytic domains.
This can result in high-dimensional datasets which are usually flattened into a
set of sample vectors, stripping the original data from its structure.
Yet, the intrinsic multiway structure of neural data~\cite{Erol2022} is
well-suited for representation as \emph{tensors}, or multiway arrays, where
each domain is represented as a tensor \emph{mode}.
Tensors provide a structured data representation for this highly dimensional
multiway data
This in turn paves the way to the development of tensor methods which can
counteract some of the drawbacks of the dimensionality problem.
Tensor methods are machine learning techniques that  consider each tensor mode
separately, reducing a given problem into partial, per-mode problems.
This approach has given rise to efficient dimensionality reduction techniques,
such as the Higher Order Singular Value Decomposition
(HOSVD)~\cite{DeLathauwer2000,SoleCasals2018}
and Canonical Polyadic Decomposition
(CPD)~\cite{Hitchcock1927,Nazarpour2006}.

HOSVD and CPD are two examples of unsupervised techniques with applications in EEG
processing.
However, given the low signal-to-noise ratio and
specific, task-related output expected in a BCI application,
supervised feature extraction and machine learning techniques are
favored\todo{cite Lotte review}.
%One approach is to incorporate assumptions of the tensor structure of the data
%directly into the estimation of parameters of classic supervised linear -
%machine learning methods, such as linear discriminant analysis (LDA) or beamforming.
%Advances have been made by leveraging the decoupling of the spatial and temporal
%domains in EEG event-related potential (ERP) classification using Spatiotemporal
%Discriminant Analysis~\cite{Li2010,Zhang2013} or methods regularizing covariance
%matrix estimation~\cite{Kerchove2022,Sosulski2022}.

\todo{come in here with François' comment about restructuring the intro.}
A more structured approach to the same problem is to design a supervised
tensor dimensionality reduction method that optimizes discriminability between the
extracted features, as is the case with Higher Order Discriminant
Analysis (HODA)~\cite{Yan2005,Phan2010,Froelich2018}.
Extracted features can subsequently be further classified, most commonly
using LDA or a support vector machine (SVM) to predict class labels.
Variants of HODA have been applied to BCI problems such as
ERP~\cite{Onishi2012,Higashi2016}\todo{cite spatiotemporal discriminant
analysis here, 'variants and special cases'} and motor imagery (MI)~\cite{Liu2015,Cai2021}
decoding.
Recent work proposes adaptations such as suited objective
functions and regularization~\cite{Jamshidi2017,Jorajuria2022,Aghili2023}.

The methods above adhere to the Tucker tensor decomposition
structure,
\todo{
To introduce BTD, I would start with a paragraph explaining the decomposition
itself, or the approximation for dimension reduction, just after the
presentation of tensors: on one side, Tucker-N (1 core only) and on the other
side polyadic decomposition (PD, m scalars) and in the middle the BTD. This
paragraph explains that in most cases the decomposition is not unique and
depends on a priori parameters (nb of cores, dimensions of projection matrices,
etc.) Also defines the residual, difference between the original tensor and the
model in the case of approximation.Then, the techniques for getting an
"optimal" decomposition or approximation could be presented: HOSVD and equiv.
(just a few refs) in the case of Tucker and CPD and others (just a few refs)
for PD. Then, your state of the art could be just focused on specific
optimizations of BTD used in BCIs (in order to justify the JNE journal
target)In fact, this is the option chosen by Axel Faes and Marc in the JNE
paper of 2022)...}
meaning that they reduce input tensors of size
$(D_1,D_2,\ldots,D_K)$ to a smaller tensor of size $(r_1,r_2,\ldots,r_K)$ with
each $r_k\leq D_k$, similar to HOSVD.
While effective, other approaches such as the PARAFAC structure employed in
CPD, where a tensor is decomposed into a sum of rank-1 tensors,
might also be suitable to represent the neural data of interest.
Discriminant tensor features can also be extracted
in the PARAFAC structure, for instance through manifold
optimization~\cite{Froelich2018}.

Nevertheless, the PARAFAC structure might still not be able to
efficiently represent all relevant information in a compressed format.
The block-term tensor structure is a generalization of the Tucker and
PARAFAC structures, and can be calculated in an unsupervised way using
the Block-term Tensor Decomposition
(BTD)~\cite{DeLathauwer2008,DeLathauwer2008a,DeLathauwer2008b,Rontogiannis2021}.
BTD, of which the HOSVD and CPD are special cases,
represents a tensor as sum of Tucker terms.
Research has shown that supervised decoders adopting this more flexible structure
can improve BCI performance.
Promising results have been achieved for regression tasks using
Higher Order Parial Least Squares~\cite{Camarrone2018} and Block-Term Tensor
Regression (BTTR)~\cite{Faes2022,Faes2022b}.
BTTR has also been adapted into a classification variant~\cite{Camarrone2021}
but this leaving room for improvement.
Instead of optimizing features directly for class separability, rather a dummy
independent variable was regressed towards, and the method
cannot be extended to a multi-class setting.
Furthermore, structures employed in HOPLS, BTTR are still more constrained than
what could be achieved with a full block-term tensor structured decomposition
optimized for discriminability, since they rely on a low rank common subspace
between the input and classification labels.
\todo{remove acronyms if not used later}

%\todo{Describe spectrum-weighted tda~\cite{Huang2020}, structure is not
%	flexible enough, not broadly applicable, not interpretable}

We propose to mitigate these issues by designing a new supervised feature
extraction tensor method based on the abovementioned HODA algorithm
that is more suited for the extraction of discriminant
features while adhering to a flexible and efficient block-term tensor
structure.
This work features the following contributions:
\begin{enumerate*}[label={\arabic*)}]
	\item  We develop a forward model for HODA to reconstruct a
	      given input tensor from the extracted features.
	\item This allows us to introduce a state-of-the-art BCI feature extraction
        method based on the block-term tensor structure, named Block-Term Tensor Discriminant Analysis
	      (BTTDA).
  \item We evaluate a BCI decoder based on BTTDA and its special
	      PARAFAC-structured case on decoding tasks for both ERP and MI
	      paradigm BCI datasets.
\end{enumerate*}
\todo{It's always trichiin in the related work section of a ML/Mathematical
paper, but unless you r reviewers are absolute experts of this specific field,
you should at least try to better describe the previous methods.}

\section{Methods}

\subsection{Notation}
Tensors are indicated by bold underlined letters $\ten{X}$, matrices by bold
letters $\mat{U}$, fixed scalars by uppercase letters $K$ and variable
scalars as lower case letters $k$.
The $n^\text{th}$ sample of a tensor dataset with $N$ samples is written as
$\ten{X}(n)$.
A tensor $\ten{X}\in \mathbb{R}^{D_1\times D_2 \times \cdots \times D_K}$ can be
unfolded in mode $k$ to a matrix
$\mat{X}_k\in\mathbb{R}^{(D_k\times\prod_{j\neq k}^K D_j)}$, by concatenating
all mode $j\neq$ fibers.
The tensor-matrix product of tensor $\ten{X}$ with matrix $\mat{U}$ along a
given mode $k$ is written as $\ten{X}\mpr{\mat{U}}{k}$. For ease of notation, let
$\ten{X}\mmpr{\mat{U}} =
	\ten{X}\mpr{\mat{U}}{1}\mpr{\mat{U}}{2}\cdots\mpr{\mat{U}}{K}$.
When skipping one of the modes $k$, this is
written as $\ten{X}\mmprs{\mat{U}}{k} =
	\ten{X}\mpr{\mat{U}}{1}\mpr{\mat{U}}{2}\cdots\mpr{\mat{U}}{k-1}\mpr{\mat{U}}{k+1}\ldots\mpr{\mat{U}}{K}$.
%The Kronecker product is noted as $\otimes$, covariance matrices are indicated  with $\mat{\Sigma}$.

\subsection{Higher Order Discriminant Analysis}
Higher Order Discriminant Analysis (HODA)~\cite{Phan2010} is a
supervised tensor-based feature extraction technique.
For a set of $N$ tensors of order $K$
$\left\{\ten{X}(n)\in\mathbb{R}^{D_1\times D_2 \times \cdots \times
		D_K}\right\}_n^N$, HODA finds projection matrices $\mat{U_k}$ for each mode $k$
that project a given $\ten{X}$ to a latent tensor
$\ten{G}\in\mathbb{R}^{r_1\times r_2\times\cdots\times r_K}$, usually with lower
dimensionality $(r_1\leq D_1,r_2\leq D_2,\ldots,r_K\leq D_K)$ using
tensor-matrix mode products
\begin{equation}
	\ten{G}  = \ten{X}\mmpr{\mat{U}}
	\label{eq:HODA-backward}
\end{equation}
visualized in Figure~\ref{fig:HODA-backward}.
\begin{figure}[t]
	\centering
	\input{figures/bttda/HODA_backward.tikz.tex}
  \caption[The \ac{hoda} backward projection.]{%
    A visualization of the multilinear projection obtained by Higher Order
    Discriminant Analysis (HODA) applied to a third-order tensor
    sample $\ten{X}$ with shape $(D_1,D_2, D_3)$.
		HODA finds projection matrices $\mat{U}_k$ such that maximal
		discriminability between classes is achieved in the projected latent tensors
		$\ten{G}$ with reduced dimensionality $(r_1,r_2,r_3)$.}
	\label{fig:HODA-backward}
\end{figure}
Analogous to the HOSVD, HODA is a dimensionality
reduction decomposition that results in a dense latent tensor $\ten{G}$, and
imposes an orthogonality constraint on $\mat{U}_k$ to ensure uniqueness.
However, while for the HOSVD decomposition the projection matrices
are chosen to minimize the reconstruction error, the projection matrices
$\mat{U}_k$ of HODA are optimized for maximal discriminability of tensors
$\ten{G}(n)$ belonging to classes with labels $c_n$.
This is a desirable property in a classification setting where samples are
high-dimensional tensors.

HODA optimizes discriminability in the Fisher sense, by optimizing the
Fisher ratio $\phi$ between the latent tensors $\ten{G}(n)$
\begin{equation}
	\phi = \frac{\sum_c^CN_c\left\lVert\bar{\ten{G}}(c)-\bar{\bar{\ten{G}}}\right\rVert_F^2}
	{\sum_n^N\left\lVert\ten{G}(n)-\bar{\ten{G}}(c_n)\right\rVert_F^2}
	\label{eq:fisher}
\end{equation}
for $C$ classes with each $N_c$ samples. $\bar{\ten{G}}(c)$ is the mean of
latent
tensors of class $c$, and $\bar{\bar{\ten{G}}}$ the mean of
these class mean latent tensors.
The goal is now to find the optimal projection matrices\todo{You should say
somewhere that the shape (r1,r2,...rK) is fixed a priori, no?}
\begin{equation}
	\left\{\mat{U}^*\right\} = \argmax_{\{\mat{U}\}}\phi
\end{equation}
which can be solved by the iterative algorithm in Algorithm~\ref{alg:HODA}.
\begin{algorithm}
  \caption[The \ac{hoda} backward solution.]{The \ac{hoda} backward solution.}
	\label{alg:HODA}
	\input{chapters/bttda/alg_HODA.tex}
\end{algorithm}
To start, $\mat{U}_k$ are initialized to orthogonal matrices, e.g. as random
orthonormal matrices, by a per mode Singular Value Decomposition (SVD),
or as the partial HOSVD of all stacked tensors in the dataset.
At each iteration, the algorithm loops through the modes and fixes all
projections but $\mat{U}_k$ corresponding to mode $k$.
It then finds a partial latent tensor
\begin{equation}
	\ten{G}_{-k}=\ten{X}\mmprs{\mat{U}}{k}
\end{equation}
Subsequently, a new projection matrix $\mat{V}_k$ can be found analogous to Linear
Discriminant Analysis by constructing the within-class scatter matrix
\begin{equation}
	\mat{S}_{-k,\text{w}} = \sum_n^N\tilde{\mat{G}}_{-k,k}(n)\cdot\tilde{\mat{G}}_{-k,k}^\intercal(n)
\end{equation}
with $\tilde{\ten{G}}_{-k}(n) = \ten{G}_{-k}(n) - \bar{\ten{G}}_{-k}(c_n)$,
and the between-class scatter matrix
\begin{equation}
	\mat{S}_{-k,\text{b}} =
	\sum_c^CN_c\tilde{\bar{\mat{G}}}_{-k,k}(c)\cdot\tilde{\bar{\mat{G}}}_{-k,k}^\intercal(c)
\end{equation}
with $\tilde{\bar{\ten{G}}}_{-k}(c) = \bar{\ten{G}}_{-k}(c) - \bar{\bar{\ten{G}}}_{-k}$,
and solving for the $r_k$ leading eigenvectors in the eigenvalue problem
\begin{equation}
	\mat{S}_{-k,\text{b}}-\varphi_k\mat{S}_{-k,\text{w}} =
	\mat{V}_k\mat{\Lambda}\mat{V}_k^\intercal
\end{equation}
with $\varphi_k=\Tr\left(\mat{U}_k^\intercal\mat{S}_{-k,\text{b}}\mat{U}_k\right)/\Tr\left(\mat{U}_k^\intercal\mat{S}_{-k,\text{w}}\mat{U}_k\right)$
using the $\mat{U}_k$ obtained in the previous iteration.
Finally, the orthogonal transformation invariant projections $\mat{U}_k$
are obtained by calculating the
per-mode total scatter matrices
\begin{equation}
  \mat{S}_{k,\text{t}} = \sum_n^N\mat{X}_k(n)\cdot\mat{X}_k^\intercal(n)
\end{equation}
and finding the $r_k$ leading eigenvectors of
\begin{equation}
	\mat{V}_k\mat{V}_k^\intercal\mat{S}_{k,\text{t}}\mat{V}_k\mat{V}_k^\intercal
	= \mat{U}_k\mat{\Lambda}\mat{U}_k^\intercal
\end{equation}
at each iteration~\cite{Wang2007}.
%and solving the generalized eigenvalue problem
%\begin{equation}
%  = \mat{\lambda}\mat{S}_{-k,\text{w}}\mat{U}_k
%\end{equation}
The iterative process halts after a fixed amount or iterations, or when the
update of each $\mat{U}_k$ is lower than a predetermined threshold.

To apply HODA in a classification setting, the projections
are first learned on a training dataset  with known class labels.
Next, these projections are used to extract latent tensors from the
tensors in the training dataset.
These latent training tensors are then reshaped (\emph{vectorized}) into feature vectors
$\vec{g} =  \vect(\ten{G})$ and used to train a decision classifier with the corresponding class labels.
At the evaluation stage, the projections learned from the training dataset are
used to extract latent tensors from an unseen test dataset with unknown class
labels, which can also be vectorized and passed on to the trained decision
classifier.

To avoid overfitting and improve performance in low sample size settings, the
HODA problem can be regularized by shrinking the partial
within-class scatter matrices~\cite{Phan2010} with a shrinkage factor
$\alpha_k$ at each step such that the eigenvalue problem becomes
\todo{make sure this appears after varphi is defined, move reference to
algorithm to later}
\begin{equation}
	\mat{S}_b^{(-k)} -
	\varphi\left[\left(1-\alpha_k\right)\mat{S}_{-k,\text{w}}+\alpha_k\mat{I}\right] =
	\mat{V}_k\mat{\Lambda}\mat{V}_k^\intercal
\end{equation}
As in Linear Discriminant Analysis, the shrinkage parameter $\alpha_k$ can
also be estimated in a data driven way in HODA~\cite{Jorajuria2022},
e.g., using the Ledoit-Wolf procedure~\cite{Ledoit2003} at every iteration.


\subsection{A forward model for HODA}
As a prerequisite to the proposed BTTDA model, we must find a
way to reconstruct the original data tensor $\ten{X}$ as accurately as possible
from $\ten{G}$ after dimensionality reduction.
This is usually referred to as finding a corresponding \emph{forward model}.
While a \emph{backward model} extracts latent sources or properties from the observed
data based on some task related criterion or on prior domain knowledge,
a forward model is a generative model that expresses the observed data in
function of these latent properties or sources that are given.
Forward models are useful for a.o.\ interpretability and data compression.
\todo{explicitly label error terms in equation, use $X\prime$ and error with
iteration sup}

The HODA projection in Equation~\ref{eq:hoda-backward}\todo{missing} is an example
of a backward model.
A straightforward and computationally efficient candidate for a corresponding
forward model is given by
\begin{equation}
	\ten{X} = \ten{G}\mmpr{\mat{A}^\intercal} + \ten{\mathbfcal{E}}
	\label{eq:HODA-forward}
\end{equation}
with \emph{activation patterns} $\mat{A}_k \in \mathbb{R}^{D_k\times r_k}$
and error term $\ten{\mathbfcal{E}}$.
\begin{figure}[t]
	\centering
	\input{figures/bttda/HODA_forward.tikz.tex}
  \caption[The \ac{hoda} forward projection.]{The forward projection for HODA. By calculating activation
		patterns $A_k$, the original tensor $\ten{X}$ can approximately be
		reconstructed from projected latent tensor $\ten{G}$. $A_k$ are chosen such
		that the variability captured in the latent tensor is maximally expressed in
		the reconstructed tensor and not in the error term.}
	\label{fig:HODA-forward}
\end{figure}
\todo{not referred to in text}
\todo{I would prefer a figure in which the Ai and G are on the left and
$\hat{X}$on the right. So, rather than using a "almost equal to" sign you could
use a standard equal signal. Finally, in the caption, you could clearly define
the original(X) and reconstructed ($\hat{X}$) tensors as well as the error term.}
\todo{explicitly state why choosing this versus full model or HOPLS}

A forward model should make sure that reconstruction error is minimized and
variation captured in the latent tensor is maximally captured by the forward
projection term $\ten{G}\mmpr{\mat{A}^\intercal}$, and not by the error term
\todo{not clear ("error is minimized" in the same sentence)}
$\ten{\mathbfcal{E}}$~\cite{Haufe2014}.
Hence, we aim to minimize the expected value of the cross-covariance between
the noise term and the extracted latent tensors
\begin{equation}
	\left\{\mat{A}^*\right\} = \arg\min_{\{\mat{A}\}}\text{E}\left[\text{vec}\left({\ten{\mathbfcal{E}}(n)}\right)\text{vec}\left({\ten{G}(n)}\right)\right]_n
\end{equation}
or, equivalently~\cite{Parra2005,Haufe2014},
\begin{equation}
	\left\{\mat{A}^*\right\} = \arg\min_{\{\mat{A}\}}\sum_n^N\left(\ten{X}(n) - \ten{G}(n)\mmpr{\mat{A}}\right)^2
\end{equation}
This least squares tensor approximation problem can be solved efficiently using the
alternating least squares (ALS) algorithm~\cite{Bentbib2022},%~\cite{Comon2009},
iteratively fixing all but one of the activation patterns such that
\begin{equation}
	\mat{A}_k = \argmin_{\mat{A}_k}
	\sum_n^N\left[\mat{X}_k(n) -
		\mat{A}_k\left(\ten{G}(n)\mmprs{\mat{A}}{k}\right)_k\right]^2
\end{equation}
at every iteration, which can be solved directly by ordinary least squares.
The activation patterns are initialized to the weights $\{\mat{U}\}$ of the
backward model.
Similar to fitting the backward model, the iterative process for the forward
model halts after a fixed amount of iterations or when the update of each
$\mat{A}_k$ is lower than a predetermined threshold.
The full algorithm to determine the HODA forward projection is listed
in Algorithm~\ref{alg:HODA-fw}.
\begin{algorithm}
  \caption[The \ac{hoda} forward solution.]{The \ac{hoda} forward solution.}
	\label{alg:HODA-fw}
	\input{chapters/bttda/alg_HODA_fw.tex}
\end{algorithm}
\todo{Reorder figures and add error term}


\subsection{Block-Term Tensor Discriminant Analysis}
After defining the forward model, we can construct our proposed block-term
tensor model.
Assuming the latent tensors $\ten{G}$
obtained by the backward projection of HODA do not achieve perfect
class separation, the error term $\ten{\mathbfcal{E}}$ in
Equation~\ref{eq:HODA-forward} should still contain some discriminative
information, which can be exploited to improve classifier
performance.
\todo{eps1 = X - G1 x {A1} still contains discriminative info, so
it is further projected onto another core tensor yieldingeps2 = eps1 - G2 x
{A2}etc. and finally you give equation (15)}
We thus extend the HODA feature extraction scheme with backward and
forward models defined in respectively Equations~\ref{eq:HODA-backward}
and~\ref{eq:HODA-forward} to Block-Term Tensor Discriminant Analysis
(BTTDA).
BTTDA finds multiple discriminative blocks, such that its forward
model adheres to the block-term tensor structure:
\begin{align}
	\ten{X} & = \sum_b^B\ten{G}^{(b)}\mmpr{\mat{A}^{(b)}} + \ten{\mathbfcal{E}}
	\label{eq:BTTDA-forward}
\end{align}
for $B$ extracted latent tensors $\ten{G}^{(b)}$ and residual error term
$\ten{\mathbfcal{E}}$.
Figure~\ref{fig:BTTDA} further illustrates the BTTDA model.
\begin{figure*}[t]
	\centering
	\input{figures/bttda/BTTDA.tikz.tex}
  \caption[A forward model for \ac{bttda}.]{A forward model for Block-Term Tensor Discriminant Analysis
		(BTTDA). BTTDA can extract more features
		than HODA by iteratively finding a latent tensor $\ten{G}^{(b)}$ in a
		deflation scheme.
		The HODA backward projection is first applied. Next, the
		input data is reconstructed via the HODA forward model and the
		difference between the two is found.
		Finally, this process is repeated with this difference as input data, until a
		desired number of blocks $B$ has been found.}
	\label{fig:BTTDA}
\end{figure*}
\todo{Explicitly mention that if restricted to rank one, it is parafac, and to
one block, it is hoda}

Since BTTDA is specified as a forward model, a backward modelling
procedure is required which finds the latent tensors $\ten{G}^{(b)}$ given $\ten{X}$ for
BTTDA to be useful as a feature extraction method.
The extracted features represented by the latent tensors $\ten{G}^{(b)}$ can be
computed through a deflation scheme summarized in Algorithm~\ref{alg:BTTDA}.
\begin{algorithm}
  \caption{\Ac{bttda}.}
	\label{alg:BTTDA}
	\input{chapters/bttda/alg_bttda.tex}
\end{algorithm}
For each block $b$, the latent tensor is extracted using the HODA backward
projection in \todo{ref equation}from a residual error term
$\ten{\mathbfcal{E}}^{(b)}$
\begin{equation}
	\ten{G}^{(b)} = \ten{\mathbfcal{E}}^{(b)}\mmpr{\mat{U}^{(b)}}
\end{equation}
This residual error term is calculated by finding the difference between the
previous error and its reconstruction after backward and forward HODA
projection
\begin{equation}
	\ten{\mathbfcal{E}}^{(b+1)} = \ten{\mathbfcal{E}}^{(b)} - \ten{G}^{(b)}
	\mmpr{\mat{A}^{\intercal(b)}}
\end{equation}
with $\ten{\mathbfcal{E}}^{(1)}=\ten{X}$.

The resulting latent tensors can be vectorized and concatenated into
one single feature vector per input tensor
\begin{equation}
	\vec{g}
	=\left[\vect\left(\ten{G}^{(1)}\right)\
		\vect\left(\ten{G}^{(2)}\right)\
		\cdots\
		\vect\left(\ten{G}^{(B)}\right)\right]
\end{equation}
so that they can be classified in a similar manner to HODA.


\subsection{Model and feature selection}
Similar to the unsupervised BTD, the performance of BTTDA is
heavily dependent on the rank $(r_1^{(b)}, r_2^{(b)}, \ldots,
	r_K^{(b)})$ and on the number of blocks $B$.
If these are not known a priori, i.e. if they cannot be set based on insights in the
data generation process, a model selection step is necessary in order to
determine what the optimal values for $r_k^{(b)}$ and $B$ are.

Furthermore, HODA, and by extension BTTDA can extract a substantial amount
of redundant features, which can be dropped after projection and before proceeding to the classification
step~\cite{Phan2010}.
Specifcally in BTTDA redundant features can stack up over the number of
blocks, hampering performance.
Relevant features can be retained by calculating the
univariate Fisher score for all features.
The Fisher score $\phi{i}$ for feature $i$ is obtained as
\begin{equation}
	\phi(i) = \frac
	{\sum_c^C N_c \left(\bar{g}_i(c)-\bar{\bar{g}}_i\right)^2}
	{\sum_n^N \left(g_i(n)-\bar{g}_i(c_n)\right)^2}
\end{equation}
The features can then either be sorted by $\phi(i)$ and a given amount of
features retained for classification, or the decision to retain a feature can
be made by a threshold on the statistical $p$-values corresponding to the
$\phi(i)$.
We opted for the latter strategy since it does not require us to redetermine the
optimal number of features to retain at each block.

Combined, this results in hyperparameters $B$ for the number of blocks, the
threshold value for feature retention, and the ranks of the individual blocks.
While we can reasonably set the feature selection $p$-value threshold to $0.05$,
$B$ and the block ranks must be tuned to select a performant feature extraction
model.
While these hyperparameters can be set through cross-validation, this can be
computationally expensive.
To reduce the computational cost of model selection,
Algorithm~\ref{alg:model-selection} proposes a heuristic model selection
algorithm that leverages cross-validation in a greedy way per block, to
iteratively find the optimal rank for the next block given the ranks of the
previous block.
The area under the receiver operating characteristic curve (ROC-AUC) for
classification of extracted feature vectors after feature selection is used as
cross-validation score.
\begin{algorithm}
  \caption[The greedy model selection procedure.]{Greedy model selection}
	\label{alg:model-selection}
	\input{chapters/bttda/alg_model_selection.tex}
\end{algorithm}
Finally, the series of blocks can be truncated to the point with
highest validation score to determine $B$.

Alternatively, a special case of BTTDA can be constructed using only rank-one
blocks such that the resulting forward model adheres to the CPD structure.
The optimal amount of rank-one blocks can be found by truncating as above.
We refer to this strategy as PARAFACDA.

\section{Experiments}
\subsection{Datasets \& decoders}
We evaluated our proposed model in two offline EEG-based BCI decoding problems:
the event-related potential (ERP) and motor imagery (MIà paradigms using a
selection of the openly available MOABB benchmarking datasets~\cite{Aristimunha2023}.
Two ERP and two MI datasets were retained to reduce computational
demand.
Details about these datasets are found in Table~\ref{tab:moabb}.
For the ERP datasets, the task is to distinguish target from non-target ERPs,
while the MI datasets consist of distinguishing different imagined limb
movements.
Within-session classification performance was assessed using stratified 5-fold
cross-validation to calculate the area under the receiver operator
characteristic curve (ROC-AUC).
\begin{table*}[t]
	\centering
	\footnotesize
	\input{chapters/bttda/moabb.tex}
  \caption[MOABB datasets used for evaluation.]{MOABB datasets used for evaluation, with the number of
		subjects (\# Sub.), the number of EEG channels (\# Chan.), the number of
    classes (\# Classes), the number of trials or trials per class for ERP
    datasets (\# Trials), the epoch length (Epoch len.), the sampling
		frequency (S. freq.), the number of sessions per subject (\# Sess.) and the
		number of runs (\#Runs).
    Adapted from~\cite{Aristimunha2023}
    and~\cite{Chevallier2024}.}
	\label{tab:moabb}
\end{table*}

To use HODA as a decoder, it is paired with LDA to classify the
extracted feature (HODA+LDA), with hyperparameters $r_k$.
Similarly, we implemented BTTDA+LDA with the proposed BTTDA feature
extraction with hyperparameters $r_k^{(b)}$ for each block $b$ and the number of blocks
$1\leq B\leq16$.
Additionally, we also introduce PARAFACDA+LDA, which is the special case of
BTTDA+LDA where each $r_k^{(b)}=1$, with $B$ and the feature selection
threshold as only hyperparameters.
Hyperparameters were determined separately for each fold using nested
stratified 5-fold cross-validation, and, for BTTDA+LDA, in conjunction with the
greedy model selection algorithm in Algorithm~\ref{alg:model-selection}.
For HODA+LDA and the HODA blocks in BTTDA+LDA, we choose
$r=r_1=r_2=\ldots=r_K$ with possible values
$\textstyle{1,2,4,8,\ldots,\max_kD_k}$
to reduce computational cost.
Other HODA and BTTDA hyperparameters were set to
$B_\text{max}=10$, $\varepsilon=\num{1e-8}$ and $I_\text{max}=128$.

Furthermore, as additional comparison methods, we used the methods evaluated in~\cite{Chevallier2024}.
For the ERP datasets, these were the Riemannian Geometry-based methods
using augmented ERP covariance matrices with and without SVD dimensionality
reduction features for a Minimum Riemannian Distance to Mean classifier
(ERPCov+MDM and ERPCovSVD+MDM), augmented ERP covariance matrices after
XDAWN~\cite{Rivet2009}
filtering paired with a Riemannian Minimum Distance to Mean classifier or a
projection to tangent space and a support vector machine (XDAWNCov+MDM and
XDAWNCov+TS+SVM) and LDA applied after XDAWN filtering (XDAWN+LDA).
For the MI datasets, the comparison methods were picked among Riemannian
methods.
These comprise of projection onto the Riemannian tangent space combined with an
ElasticNet classifier (TS+EL), a Fisher Geodesic Minimum Riemannian Distance to
Mean classifier and the classification of augmented covariance matrices
projected onto the Riemannian tangent space with a support vector machine
(ACM+TS+SVM).
Furthermore, we report the performance of the Common Spatial Patterns filter
and a frequency filterbank combined with a support vector machine (CSP+SVM and
FilterBank+SVM respectively).
We refer to~\cite{Chevallier2024} for implementation details of these comparison
methods.

\subsection{Event-Related Potentials}
ERPs are spatiotemporal features, with each sample forming a $2^\text{nd}$
order tensor with $K=2$ modes (a matrix), representing EEG channels and time samples
per epoch.
EEG signals for the evaluated datasets were recorded at the sample rate given
by Table~\ref{tab:moabb} and band-pass filtered between 1Hz
and 24Hz.
The signals were cut into epochs starting from stimulus onset with a
dataset specific length given by Table~\ref{tab:moabb}.
For HODA+LDA, PARAFAC+LDA and BTTDA+LDA decoders, epochs were downsampled to 48Hz.
For the BNCI2014-008 and BNCI2015-003 datasets, this resulted in matrices of
dimensionality $(8,48)$ and $(8,38)$ respectively.

Table~\ref{tab:erp-score} lists the cross-validated ROC-AUC for all evaluated
decoders.
\begin{table}[t]
	\footnotesize
	\centering
	\input{chapters/bttda/score_erp.tex}
  \caption[Whithin-session classification score for 2 event-related potential
  datasets.]{Area under the receiver operating characteristic curve for
		cross-validated whithin-session evaluation for HODA and our proposed decoders
		PARAFACDA and BTTDA evaluated on 2 event-related potential datasets.
    Scores for other decoders were taken from \cite{Chevallier2024}.
		BTTDA reaches the highest performance for both evaluated datasets, closely
		followed by PARAFACDA.
	}
	\label{tab:erp-score}
\end{table}
Highest performance is reached with the proposed BTTDA+LDA.
One-sided Wilcoxon signed-rank tests with significance level $\alpha=0.05$ reveal that both PARAFAC+LDA and BTTDA+LDA
significantly outperform HODA+LDA in both the BNCI2014-008 (PARAFAC+LDA:
$p=0.0039$, BTTDA+LDA: $p=0.0039$) and the BNCI2015-003 (PARAFAC+LDA:
$p=0.0001$, BTTDA+LDA: $p=0.0049$) datasets.
No significant difference was found between BTTDA+LDA and PARAFACDA+LDA.

\subsection{Motor Imagery}
For MI, discriminatory information is represented in the EEG data as
Event-Related Synchronizations/Desynchronizations (ERS/Ds).
Contrary to the time domain analyses performed on ERPs, ERS/Ds are usually well
discerned in the time-frequency domain.
Hence, for the MI task, we will transform the EEG signal into the
time-frequency domain, forming $3^\text{d}$ order tensors, with $K=3$ modes
representing the channels, frequencies and time bins.

The MI datasets listed in Table~\ref{tab:moabb} were first band-pass filtered
between 8 and 30Hz and cut into
epochs with time windows as specified by Table~\ref{tab:moabb}.
Next, time-frequency transformation was performed using a complex Morlet wavelet
convolution, with 16 wavelet frequencies logarithmically spaced between 8 and
32Hz.
The number of wavelet cycles $c$ varied with wavelet frequency $f$ as
$c=0.7*f$.
Features were extracted by taking the log-transformed envelope of the complex
wavelet transformation an daveraging each epoch along the time axis into time bins of
length $1/4$s.
For the BNCI2014-001 and BNCI2014-004 dataset, this resulted in tensors of dimensionality
$(22, 4, 16)$ and  $(3, 4, 18)$
respectively.

Table~\ref{tab:erp-score} lists the cross-validated classification score for
the evaluated motor imagery datasets.
Note that, in line with~\cite{Chevallier2024}, accuracy is reported for the
multi-class classification problem in BNCI2014-001, while ROC-AUC was reported for the
binary classification problem in BNCI2014-004.
\begin{table}[t]
  \centering
	\footnotesize
	\input{chapters/bttda/score_mi.tex}
  \caption[%
    Whithin-session classification score for 2 motor imagery datasets.
    ]{Classification score for
		cross-validated whithin-session evaluation for HODA+LDA and our proposed decoders
		PARAFACDA+LDA and BTTDA+LDA evaluated on 2 motor imagery datasets.
		Scores for other decoders were taken from \cite{Chevallier2024}. Accuracy
    is listed for BNCI2014-001 and area under the receiver-operator
    characteriscic curve for BNCI2014-004. BTTDA outperforms HODA and PARAFACDA
    for BNCI2014-001 but does not reach a performance comparable to current
    state-of the art. For BNCI2014-004, PARAFACDA and BTTDA perform
    approximately on par with HODA and the current state-of-the art.}
	\label{tab:mi-score}
\end{table}
For the BNCI2014-001 dataset, HODA+LDA and our proposed decoders do not reach
satisfactory performance compared to the comparison methods, yet here both
PARAFACDA+LDA and BTTDA+LDA improve upon HODA+LDA, and for BTTDA+LDA this
difference is significant ($p=0.0269$).
For the BNCI2014-004 dataset the performance gap with comparison methods is
smaller, but no significant differences were found between HODA+LDA,
PARAFACDA+LDA and BTTDA+LDA.

\subsection{Block contribution}
\todo{Discuss number of features}
To analyze the contribution of extra feature blocks extracted by {BTTDA} over
the first one found by HODA we pick n ERP ($K=2$) dataset
(BNCI2014-008) and an MI ($K=3$) dataset (BNCI2014-001).
We report within-session ROC-AUC scores for training, validation and test data as a function
of the number of blocks increases, Figure~\ref{fig:blocks}.
Training and validation folds were taken from the model selection procedure.
Additionally, the Normalized Mean Squared Error (NMSE) is reported for the
reconstructed from the truncated BTTDA decomposition
$\textstyle{\ten{X}_\text{rec}^{(B)}=\sum_b^B\ten{G}^{(b)}\mmpr{\mat{U}^{(b)}}}$.
NMSE is calculated as
\begin{equation}
	\nmse\left(\ten{X}, \ten{X}_\text{rec}^{(B)}\right) =
	\frac{\sum_n^N\left\lVert\ten{X}(n)-\ten{X}_\text{rec}^{(B)}(n)\right\rVert_F^2}
	{\sum_n^N\left\lVert\ten{X}(n)\right\rVert_F^2}
\end{equation}
\begin{figure*}[t]
	\input{figures/bttda/blocks.pgf}
  \caption[Analysis of NMSE and classification score per block.]{%
    Normalized Mean Square Error (NMSE) (left), and difference in
    classification score for the training and validation for the ERP datasets
    (top row) and the MI dataset (bottom row)	of the greedy model selection
    procedure (right), as a function of the number of BTTDA blocks $b$.
		While NMSE monotonically decreases for the evaluated datasets, better class
		separation will be achieved, but eventually overfitting occurs and validation
		and test scores will drop, or plateau due to feature selection.
	}
	\label{fig:blocks}
\end{figure*}
\todo{format figure}


\section{Discussion}
\todo{subtensors in larger tensor drawing, focus more on sparsity in the
discussion}
\todo{focus on tradeof: more flexibility in model, but this implies more
hypermarameters so it relies more on a model selection procedure.}
The results in Table~\ref{tab:erp-score} and Table~\ref{tab:mi-score} show that
in the ERP datasets, BTTDA+LDA and PARAFACDA+LDA reach state-of-the-art decoding performance by
exceeding all comparison methods for the two evaluated datasets, but
for MI datasets results fall short of those of comparison methods.
Performance on the MI classification task could however be greatly influenced
by the exact tensorization method used, i.e. the time-frequency transform in
this case, which is not the main focus of this work.

Nevertheless, when considering at the relative improvement over the original
HODA model, our results show BTTDA+LDA performs consistently on par or higher than HODA+LDA.
The classification score for BTTDA+LDA was also slightly higher than that of
restricted PARAFACDA+LDA for the ERP datasets and one of the MI datasets, but these
results were not significant and further studies with more datasets and
subjects should show whether this holds.
Figure~\ref{fig:blocks} shows that there is added value in finding extra blocks
over the first HODA block.
While no proof is given, we notice NMSE monotonically decreases, indicating that
Since the training score keeps increasing, this indicates that
eventually all the variation in the signal will be explained by the model
while still extracting features that are maximally discriminant.
Eventually, the amount of blocks will reach a point of diminishing validation
score returns, when adding extra features to the decision classifier increases
its risk of overfitting instead of adding extra useful discriminatory
information.
Together with the improved classification scores presented, these results
point to the potential of our more flexible model block-term or
its special PARAFAC-structured case over a Tucker-structured model given proper block and
rank selection.
Since the optimal ranks for HODA+LDA were also determined through
cross-validation, BTTDA+LDA can improve over the first HODA block.
If validation shows that this is not possible, such as is the case for
BNCI2014-004, the BTTDA model is truncated to the first HODA block and little
performance is lost.

We assume the main benefit of BTTDA is that it can more easily discover relevant
features, while
being more parsimonious due to its block-term structure compared to HODA's full
Tucker structure.
Alternatively, the enhanced performance could also stem from the modeled data
covariance.
Since HODA estimates one whithin-class scatter matrix
$S_{-k,\text{w}}\in\mathbb{R}^{D_k\times D_k}$ per mode, its overall
model of the data scatter is determined by these per mode scatter matrices as a
Kronecker product $S_{-1,\text{w}}\otimes S_{-2,\text{w}}\otimes\cdots\otimes S_{-K,\text{w}}$, which corresponds to the assumption that the EEG data is
drawn from a multilinear normal distribution~\cite{Ohlson2013}.
However, it is known that the EEG covariance cannot fully be expressed as a
single Kronecker product, but rather is more accurately modeled by a sum of
multiple Kronecker products~\cite{Bijma2005, Sosulski2022}.
Since BTTDA iteratively fits HODA models to the residual error, it will be able
to express the full covariance structure given sufficient blocks.

Additionally, the forward modeling step inherent to the BTTDA results
in an interpretable model, since we can use the activation patterns or the
forward projection to inspect the neural patterns corresponding to the
relevant discriminatory information at each block~\cite{Haufe2014}.
Figure~\ref{fig:forward} shows the activation patterns
of two blocks obtained from the BNCI2014-008 dataset as well as the forward
projection of the difference between the averages of the mean latent tensor per
class (\emph{contrasts}) after forward projection.
While the weights of the backward projection are
uninterpretable~\cite{Haufe2014},
the activation patterns and contrasts after forward projection clearly show
that ERP components can be recognized and separated into different
BTTDA blocks.
\begin{figure*}[t]
	\includegraphics[width=\linewidth]{figures/bttda/forward_block-0.png}
	\includegraphics[width=\linewidth]{figures/bttda/forward_block-1.png}
  \caption[Extracted \ac{bttda} activation patterns.]{%
    Spatial (left two columns) and temporal (middle column) activation patterns and
		condition contrasts (right column) obtained after forward projection of the latent
		features for 2 blocks (row 1 and row 2) of rank $(2,2)$ of BTTDA
		fit on the full dataset BNCI2014-008.
		The separate blocks approximately model different ERP
		components.}
	\label{fig:forward}
\end{figure*}
Given informed or correctly tuned hyperparameters, this method could be used to
e.g. separate ERP components or neural processes based on the task-related
information in the class labels.


Despite favorable results in BCI decoding, the applications of the proposed
BTTDA model are limited mainly by the model selection approach used
to determine the individual block ranks.
While our proposed greedy model selection algorithm is a step in the right
direction, the high computational cost of setting hyperparameters through cross
validation can still hinder the portability of decoders relying on
BTTDA.
Due to its heuristic nature, the greedy algorithm does not always result in the
set of ranks with the highest achievable performance.
In combination with the fact that the feature selection cutoff parameter was
fixed somewhat arbitrarily, it is clear that thorough
hyperparameter optimization could improve performance.
Furthermore, it is clear that our proposed model selection procedure does not
necessarily result in an optimal set of blocks that group coherent projections
within the same block, according to some some desirable metric.
Examples of this are sparsity, pattern interpretability, minimal or maximal between- or within-block feature
correlation, decreasing discriminability etc.
Future efforts should focus on automatic parameter setting, e.g. using
information criteria such as the ones used in BTTR~\cite{Faes2022} or other
statistical measures based on the models application.
\todo{comment as limitation that we test r1=r2=r3}

Another limitation is that BTTDA might yield a disproportionate
improvement for datasets with a low number of features relative to sample size,
while being less effective for datasets with more features
This is reflected
in our ERP results (low dimensionality vs. high number of trials) compared to the
MI results (higher dimensionality due to third order tensorization vs. lower
number of trials).
We expect a dimensionality limit beyond which the forward modeling step cannot
accurately regress from the low dimensional latent tensors to the high
dimensional original tensors, introducing
error in the input data for the next block, which can stack up over blocks.
Since the forward multilinear least squares problem is underdetermined, it is prone to
numerical instability, which calls for regularization of the forward modeling
procedure, but this would introduce another hyperparameter.
It should also thoroughly be investigated what the impact is of going beyond
second- and third-order case to higher order tensors, since this could have a large impact
on the model.
Other tensorization methods of the EEG data, like time-lagged Hankel
tensors~\cite{Papy2005}, or tensors across subjects or sliding windows etc., could also be
of interest if they are appropriately chosen based on prior knowledge of the dataset.
\todo{mention in discussion of model selection procedure that sparser blocks
would be better}

\section{Conclusion}
\todo{main restriction for hoda is model selection. this is also a model
selection problem, but it is more flexible}
We have introduced Block-term Tensor Discriminant Anlysis (BTTDA) a novel,
tensor-based, supervised dimensionality reduction technique optimized for class
discriminability which adheres to the block-term tensor structure.
BTTDA is a generalization of Higher Order Discriminant Analysis and can also be
applied as a special sum-of-rank-one tensors PARAFACDA model.
The model is obtained by iteratively fitting HODA in a deflation scheme,
leveraging a novel forward modeling step.
Via an accompanying heuristic model selection procedure, BCI decoders using BTTDA
feature extraction can significantly outperform decoders based on HODA and
reach state-of-the art decoding performance on event-related potential
problems (second order tensors) and scores on par with or higher than HODA  in motor
imagery problems (third order tensors).
Moving from the rigid Tucker tensor structure of HODA to the more flexible
block-term structure shifts the problem from finding optimally constrained multilinear
projections to model and feature selection.
This allows performance to be traded
off for model complexity and number of features, to find a setting that is more
effective for decoding.
Because of the flexibility and minimal assumptions BTTDA
can equally be applied to other neuroimaging modalities (MEG, ECoG, fNIRS,
fMRI, EMG, ...) or tensor classification problems in other fields.

\section*{Acknowledgements}
We thank the Flemish Supercomputer Center (VSC) and the High-Performance
Computing (HPC) center of the KU Leuven for allowing us to execute our
computational experiments on their systems.
We also wish to acknowledge dr.\ Axel Faes for his inspiration in conceptualizing this
work.
