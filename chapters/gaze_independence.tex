\emph{Parts of Section~\ref{sec:gaze-independence} have been published in \textcite{VanDenKerchove2024}.}

\todo{spatially vs temporally organized interfaces}
\todo{talk about covert shifts in spatial attention alpha}

\section{The gaze-dependence problem}
\label{sec:gaze-dependence}

\todo{define eye motor issues}

\todo{Algemeen: in begin uitleggen dat de ziektes er niet veel toe doen en dat
het gaat over verlamde personen, verlies van spierkracht en coördinatie en dus
gehinded in communicatie, helemaal in begin van chapter 1 duidelijk maken dat
het om deze patiënten gaat}
One of the goals of brain-computer interfacing is establishing a communication
channel that does not rely on speech or muscular activity~\cite{Naci2012,Chaudhary2016},
which in turn can provide solutions to paralyzed individuals.
In the strictest interpretation, this also means that an interface should not
rely on the control of eye muscles used to redirect the gaze or for blinking.
\todo{patients suffering paralysis due to damage to the CNS (stroke, spinal
chord injury, tbi) or neurodegenerative diseases}

In fact, many patients in the BCI target population suffer to some extent
from eye motor disability, requiring BCIs adapted to their condition.
Table~\ref{tab:incidence} reports the relatively high frequencies of
eye motor problems, which can range from minor (nystagmus
\footnote{
Involuntary, rhythmic, and repetitive eye movements recognizable by their
consistent directionality (horizontal, vertical, or rotational).
}, other eye tremors
\footnote{
These can include square-wave jerks, saccadic intrusions, microtremors, or
microsaccades while resting or fixating the gaze.
}, gaze fixation fatigue or discomfort, \ldots), to severe (partial ophtalmoplegia
\footnote{
Weakness or limited paralysis of one or more of the muscles that control eye
movement, leading to restricted eye motion, but not complete paralysis.
Often, a specific movement direction (up-down, left-right) is preserved.
}, involuntary movements, impaired pursuit, \ldots), and even complete
ophtalmoplegia\footnote{Full eye movement paralysis.} or eye motor paresis.
This not only has an effect on vision and coordination but also
on their ability to operate a visual \ac{bci}.

\begin{table}[h]
  \sffamily
  \footnotesize
	\centering
	\begin{tabular}{@{}l|rrrrr|r@{}}
    & \bfseries \acs{als} & \bfseries \acs{ms}   & \bfseries Stroke &\bfseries\acs{dmd} &\bfseries \acs{sma} &\bfseries \acs{lis} \\ \hline
		\bfseries Minor    & 50\% & 31\% & 40-70\% & + & - &      \\
		\bfseries Severe   & 33\% & 3\%  & +       & - & - & 98\% \\
		\bfseries Complete & 17\% & -    & +       & - & - & 2\%  \\
	\end{tabular}
	\caption{Incidence of eye motor impairment in selected BCI target
    patient populations. (\acs{als}: \acl{als}, \acs{ms}: \acl{ms}, \acs{dmd}:
    \acl{dmd}, \acs{sma}: \acl{sma}, \acs{lis}: \acl{lis}).
    $+$: frequent, $-$: infrequent.}
    \label{tab:incidence}
\end{table}
\todo{include FA}

Among the most affected patients are stroke patients~\cite{Pollock2011}, mostly
due to brainstem or cerebellar stroke~\cite{Moncayo2009,Bogousslavsky1987}.
\todo{summarize and cite Rowe2019}
These patients might suffer from severe or complete eye motor impairment
immediately from the onset of their condition.
In \ac{als}, a progressive disease, oculomotor issues are also fairly common.
Although eye movement is often cited as one of the longest preserved
capabilities in \ac{als}, \textcite{Guo2022} show that minor issues are still
fairly common,
especially for patients with bulbar onset \ac{als}.
Furthermore, \textcite{Hayashi1991} show that, as these patients are supported beyond
respiratory failure, the disease progression will eventually also involve
paralysis of the eye muscles.
Various forms of eye movement abnormalities also occur often and are especially well studied in
\ac{ms}~\cite{Mueri1985,Prasad2010,Castelnovo2016,Serra2018,Polet2020}.
These abnormalities can be minor or severe, seldom progressing to complete
paralysis.
However, \ac{ms} often comes with vision loss, further complicating interaction
with visual \acp{bci}.
\todo{FA}
Patients suffering from other neurodegenerative diseases like
\ac{sma}~\cite{Anagnostou2021} and \ac{dmd}~\cite{Lui2001} are
sometimes also interested in \ac{bci} use, but their eye motor capabilities are
mostly preserved.

\newcommand\fnlis{\footnote{
Multiple definitions of \ac{lis} are encountered in
BCI and neurological literature.
Some definitions include only tetraplegic patients without eye movements used
for communications.
Others distinguish Complete Locked-in Syndrome (CLIS) with full body paralysis,
including no eye motor control at all, from a \ac{lis} state with some preserved eye
movements or minor motor output.
While some definitions only include patients who suffered stroke or traumatic
brain injury to specific regions in the brain (midbrain, brainstem, or
cerebellum)~\cite{Smith2005},
it can also generally refer to the state of full body paralysis
or loss of muscle tone incurred in neurodegenerative diseases, combined with
the inability to speak, such as occurs in late stage ALS.
In clinical reality, every tetraplegic patient has their own unique set of
preserved capabilities and from an solution-oriented \ac{bci} engineering point
of view, the etiology of the symptoms can be abstracted away.
}}

\newcommand\fnwolpawcrit{\footnote{
\it``The first class consists of people who are truly totally locked-in (e.g.,
due to end-stage ALS or severe cerebral palsy), who have no remaining
useful neuromuscular control of any sort, including no eye movement.
\elide\ This class is very small. \elide\
The second class of potential BCI users comprises those who retain
a very limited capacity for neuromuscular control. This group includes
people who retain some useful eye movement or enough limb muscle
function to operate a single-switch system. Such control is often slow,
unreliable, or easily fatigued.
This group is much larger than the first.
\elide\
The third class of potential BCI users, which is the largest of all,
includes those who still retain (and can be expected to continue to
retain) substantial neuromuscular control, particularly speech and/or
hand control, and can, therefore, operate a wide range of assistive
communication and control devices.''
}}


Together, some of these patients constitute the \ac{lis} patient group, which
forms one of the main \ac{bci} target populations.
In this work, we'll define \ac{lis} patients as those in a situation of (near)
complete paralysis and difficulties or the inability to communicate without
assistive technology
This corresponds to classes one and two defined by
\textcite{Wolpaw2006}\fnwolpawcrit.
Additionally, some degree of severe or complete eye motor impairment is usually
necessary to qualify as locked-in\fnlis.
Since this work is concerned with gaze-impaired patients, we will adopt this
interpretation into our definition.
Case study reports~\cite{Patterson1986} show that the population of locked-in
patients with complete eye motor paralysis is very small.
Hence, it would be interesting to focus \ac{bci} development efforts on the
larger population of \ac{lis} patients with severe eye motor impairment that
currently slip through the cracks of the assistive technology offer.
These are the patients whose severe eye motor impairment prevents them from
using eye tracking based solutions.
They currently use their remaining motor control to
communicate by indicating symbols on a letterboard with great effort,
or signal with upwards eye movements or blinks to confirm prompted letters.
They require a caregiver or relative to interpret their signals, and crave
the ability to communicate independently which is crucial to retaining an
acceptable quality of life.
\todo{order of concepts in next two paragraphs can be tweaked}

This leads us to what we'll call the \emph{gaze-dependence problem in visual
BCI}:
Traditional visual BCI scenarios require the user to overtly direct their both
\ac{vsa} and gaze toward the screen target they intend to select.
For our purposes, screen targets are overlaid with non-overlapping, transient
stimuli that evoke event-related  potentials \acp{erp} in the \ac{eeg}.
The selected target can be decoded from these \acp{erp}, as is the case for the
oddball paradigm where observing a rare but attended stimulus evokes a P3 ERP
component.
However, a critical challenge arises when users rely solely or in part on covert
VSA, which involves directing visuospatial attention without corresponding eye gaze.
In these cases, classical solutions often fall short of the widely accepted
80\% target selection accuracy threshold~\cite{Brunner2010,Frenzel2011,Treder2010,Ron2019} deemed necessary for a comfortable user
experience~\cite{Neeling2019}, calling for alternative, gaze-independent
solutions.

In this work, we will use the term \emph{gaze-independent} meaning
\emph{‘dealing explicitly with the fact that a user cannot control their
gaze.'}
In the context of a visual BCI, this means that the user's
visuospatial attention and their gaze do not necessarily coincide.
For patients with eye motor impairment, gazing directly at a screen target may
be uncomfortable, impractical, or even impossible.
Hence, assistive devices that rely on eye tracking are often inefficient for
them.
Consequently, while BCIs hold great promise for these individuals, conventional
gaze-dependent BCI solutions do not meet their needs due to the absence of gaze
control. Therefore, the development of decoding strategies that account for covert
VSA becomes crucial in the pursuit of high-performance gaze independent
BCIs.



\section{Gaze-independent BCI paradigms}
\label{sec:gaze-independence}

Gaze-independent ERP-based BCIs~\cite{Riccio2012, Aloise2012} can be realized in three
ways.
Firstly, active BCI communication paradigms relying on endogenous activation from the user
do not rely on sensory stimulation.
Examples of this are imagined movement or imagined speech paradigms.
These paradigms can yield very high information transfer
rates~\cite{Willett2021,Metzger2023}, both due to their intuitiveness and the
complexity that can be captured in commands, but often only do so when paired
with invasive recording.
Non-visual reactive paradigms that use e.g. auditory and somatosensory
stimulation do not rely on gaze redirection exist but also result in lower information transfer
rates compared to visual paradigms.
Furthermore, they can suffer from increased mental effort in operation and user-dependent variability~\cite{Reichert2020b}.
\todo{insert table from riccio in text as numbers, not as table}

Secondly, current visual stimulation paradigms can be optimized, e.g. such that
the stimuli are always present in the field of view either overtly~\cite{Acqualagna2013, Won2018,
Lin2018} or covertly~\cite{Treder2010,Pires2011,Lees2018}. Some noteworthy examples include the GIBS Block Speller~\parencite{Pires2011},
the GeoSpell interface~\parencite{Aloise2012}, and the RSVP
speller~\parencite{Acqualagna2011}.
Non-spatial visual attention (feature attention) can also be exploited, such as
attention to stimulus color, shape or symbol~\cite{Zhang2010,Treder2011,Hwang2015}.
Alternative visual stimulation paradigms can modulate specific ERP components
that are more sensitive to stimulation in the visual periphery~\cite{Schaeff2012,Xu2022}.
Nevertheless, these methods, can suffer too from reduced information
transfer rates~\cite{Chennu2013}.
Furthermore, these systems typically require users to focus on a central fixation point while
selecting peripheral targets.
This entails that they still rely to some extent on
eye motor control, often necessitating central gaze fixation.
\todo{advantages disadvantages rewrite (requires gaze fixation, low itr, not tested with split attention)}

Thirdly, stimuli can be presented in a standard BCI paradigm, but visuospatial
attention can be decoded separately from gaze direction.
\cite{Aloise2012b} aimed to bridge the performance gap between covert and
overt VSA decoding performance.
They compared classical linear and non-linear ERP classifiers on a covert
attention P3 ERP component dataset.
The results revealed no significant performance improvement in covert VSA
decoding for any of the investigated decoders.
More recent work has made advances in decoding lateralized covert
VSA harnessing the N2pc ERP component~\cite{Thiery2016,Reichert2020b,Wang2022},
decoding covert VSA shifts from spectral content~\cite{Tonin2013}, or hybrid
stimulation paradigms~\cite{Egan2017}.
Since these methods often require a slower stimulation pace than the classical
ERP BCI paradigm, gaze-independent decoding in the fast-paced reactive ERP paradigm
leveraging spatial attention remains underexplored.

\todo{check funding proposal}
\todo{check provisional doctoral plan}
\todo{check WIP seminar}

\section{Problem statement}

\subsection{Objectives}

Brain-Computer Interfaces (BCIs) decode brain activity with the aim to establish a direct communication
pathway bypassing speech and other forms of muscular activity. They have raised great hopes for patients
devoid of these abilities, for whom BCIs can provide a means to communicate or to
control devices.
The quest for performant and affordable solutions is most evident in
visual BCIs based on the electroencephalogram (EEG), where it has led to a gamut of increasingly sophisticated decoders
and paradigms tailored to the needs of specific stimulation paradigms and
use contexts.
An effective and proven method is the visual event-related potential (ERP)-based
oddball interface.
Here, targets are shown in short pulses on a computer screen.
Each time the user observes a target, an ERP is evoked, the presence of which can be
detected in the EEG-signal.
The ERP consists of multiple components, of which some are modulated by the
attention of the user, specifically the N2 and the P3 component.
Decoding this modulation allows for information transfer controlled by the
user's brain activity, as sketched in Figure~\ref{fig:bci}.
However, visual BCIs cannot operate efficiently when the user does not direct
their gaze onto the desired target~\cite{Brunner2010, Frenzel2011}.

This leads to the often unadressed gaze-dependence paradox: the BCI target
population consists of patients who suffer from various degrees
of paralysis, or even from Locked-in Syndrome (LiS), the complete loss of muscle
control with preserved consciousness.
While BCIs are most attractive as a solution for these patients
compared to other assistive technologies like eye-tracking, studies often
report bad performance in patient populations precisely due to the lack of adequate eye motor
control, or patients with gaze impairments are excluded.
Different degrees of eye motor impairment can render visual BCIs
uncomfortable or outright impossible to use.
When operating a visual BCI, the usual rapid series of forced saccades followed
by fixation is tiring over time, even for healthy control subjects.
A suitable alternative would allow the user to keep their eyes in an at-rest
position of their choice while operating the BCI.
This points to the need for gaze-independent BCIs.

The general goal of this PhD is to tackle the gaze-dependence in visual BCIs.
\todo{refer to ITRs from previous chapter}
Section{sec:gaze-independence} showed that visual BCI paradigms form
a good candidate for a performant gaze-independent BCI.
While the central gaze fixation of most visual gaze-independent paradigms still
relies to some extent on eye motor control, we aim to circumvent this.
We make a distinction here between spatially organized interfaces, where
multiple targets are displayed at the same time at different spatial locations,
and temporally organized interfaces, like Rapid Serial Visual
Presentation\todo{cite},
where targets or small sets of targets are shown consecutively.
Traditionally, spatially organized interfaces have a higher information transfer
rate, but are also more gaze-dependent than temporally organized interfaces.
In such an interface, stimuli can be presented in a standard BCI paradigm, but
visuospatial attention can be decoded separately from the position of gaze,
which do not necessarily need to coincide.

This leads us to adopt a specific approach: improve the performance of a
spatially organized visual oddball ERP-based brain
computer interface by using a suited decoding strategy.
Hence, our working hypothesis is as follows:
\begin{quote}
	\textit{A spatially organized visual oddball ERP BCI with a suited decoder
		can achieve a similar transfer rate in patient
		populations with eye motor deficits than other gaze-independent alternatives
    described in literature.}
\end{quote}

To achieve our goal, we innovate on decoder development and interface design,
and we collect data from healthy control subjects, which allow us to test our decoders.
Finally, the findings of these experiments will be leveraged to develop a BCI
for patient use, which will be tested in a case-based patient study in which
both performance and user comfort will be evaluated.

This specific hypothesis is embedded within a broader goal of enabling
effective communication for eye-motor impaired \ac{lis} patients.
Ultimately, we wish to design a comfortable interface that allows them to maximally exploit
their residual gaze capabilities, by leveraging a non-invasive, high-\ac{itr}
BCI on the one hand, and on the other hand improving \ac{erp} decoding performance
in general.

\section{Approach}

\subsection{Interface design}

\todo{mention, free, split, frenzel}
\todo{Overt, covert, split, free, illustration from poster}
\todo{clear definitions, table from paper}

First, we designed a visual oddball interface that helps us study and adapt
to the effects of eye motor impairment on the ERP, when using existing state-of-the-art
decoders and on our own proposed decoders.
Using a hexagonal lay-out interface, similar to the visual Hex-o-Spell proposed
\textcite{Treder2010}, we present six flashing circular targets
(without letters or symbols) to the participant while the EEG and
electro-oculogram (EOG) are recorded, as well as eye gaze using eye tracking.

The interface can be operated in different VSA conditions as illustrated in
Figure~\ref{fig:gaze-vsa}.
\fourpanefig
  {\figpane{figures/gaze_independence/attention_overt.eps}{
    \emph{Overt \ac{vsa}}.
    Gaze and \ac{vsa} coincide on a target.}{fig:gaze-vsa-overt}}
  {\figpane{figures/gaze_independence/attention_covert.eps}{\emph{Covert
  \ac{vsa}}.
  Gaze rests on the center of the screen, while \ac{vsa} is directed towards a
  target.}{fig:gaze-vsa-covert}}
  {\figpane{figures/gaze_independence/attention_split.eps}{
    \emph{Split \ac{vsa}}. \ac{vsa} is directed towards a target, while the
    gaze rests on another.}{fig:gaze-vsa-split}}
  {\figpane{figures/gaze_independence/attention_free.eps}{\emph{Free \ac{vsa}}.
  The user is free to direct their gaze as they deem most comfortable
  All previous \ac{vsa} conditions are possible.}{fig:gaze-vsa-free}}
  {\Ac{vsa} conditions defined in our hexagonal spatial \ac{erp} paradigm interface.}
  {fig:gaze-vsa}

In the overt case, users gaze at the cued
target they are also mentally attending; in the covert case, users
gaze at the center of the screen while mentally attending to the cued target.
Finally, we introduce a VSA condition that is understudied in the context of
gaze-independent BCI development: split VSA.
In split VSA, the user mentally focuses on one cued target while gazing at
another (the distractor).

The option to dissociate gaze and visuospatial attention will allow us to
investigate the effect of gaze control on \ac{bci} performance.

\subsection{Decoders}

During this PhD, we explored different lines in decoding strategies,
trying to tackle several problems that arise from gaze-independence, such as
the lack or decrease in amplitude of specific ERP components, and the increased
non-stationarity of the signal.
As mentioned, state-of-the-art decoders have poor performance in covert attention
settings.
The general goal is thus to design a machine learning classifier that represents
the ERP signal in a specific way, such that it becomes more robust to the problems
occurring in covert attention conditions.

\subsubsection{Regularized spatiotemporal beamforming}
Due to the lack of the N2 component and decrease of amplitude of the
P3 component in covert attention
settings~\cite{Treder2010}, the signal-to-noise ratio (SNR) of
the ERP is lower than in overt attention settings.
Therefore, a straightforward way to reach satisfactory
gaze-independent decoding performance, might be by increasing overall ERP
decoding performance. A more accurate classifier could yield relative
improvements in those settings where performance is not yet near the achievable
maximum.

Therefore, we have improved upon an in-house developed, state-of-the art ERP
decoder, the spatiotemporal beamformer~\cite{Wittevrongel2016}, by reformulating
this classifier as a linear discrimination problem and
imposing regularizing constraints by structuring the noise covariance matrix
(STBF-struct).
Furthermore, these regularizing constraints impose temporal stationarity on
the background noise, which is of use in our next efforts to cope with the
non-stationarity of the P3 signal component.

\subsubsection{Classifier-based Latency Estimation with Woody iterations}

P3 latency generally falls between 350ms and 600ms~\cite{Luck2014}, but this
value is heavily dependent on the subject and the task, and can vary from trial
to trial~\cite{Ouyang2017}.
The work of \cite{Arico2014} illustrates that the variation in single-trial P3
latencies is important in gaze-independent decoding and has been hampering covert
VSA decoding performance.
In this work we aim to reprise their hypothesis stating that jitter compensation improves
covert VSA performance and extend it by developing a decoder and evaluating it
in a broader range of gaze-independent settings, including split VSA.

\todo{check overlap}

Literature shows that better covert attention performance can be achieved by
counteracting the non-stationarity of the P3 ERP component in covert
attention settings~\cite{Arico2014}.
This can be done by estimating the latency of each single-trial ERP and aligning
the P3 peaks to all fall at the same moment.
The resulting aligned data and the set of latencies can then be used to train a
classifier that is more robust to jitter.

Existing latency estimation methods are either not applicable to the
classification problem of labeling unseen data, or are not robust enough to
deal with the low SNR of the ERP.
Classifier-based Latency Estimation (CBLE)~\cite{Mowla2017} is a technique that can leverage
ERP latency estimation in a decoding setting, but our results show that it
yields no improvement in gaze-independent settings.
We improved upon this technique and extended it to a probabilistic, iterative
method named Classifier-based Latency Estimation with Woody iterations (wCBLE).

\subsubsection{Tensor discriminant analysis}

\subsection{Experiments}

\subsubsection{Healthy control study}
In a first series of experiments, we recorded data with this interface from healthy participants.
The goal of these experiments is to benchmark
gaze-independent ERP decoding algorithms.
We denote this dataset as the Covert Visuospatial Attention ERP dataset
(CVSA-ERP)
This study was approved by the Ethics Commission of University Hospital Leuven
(S62547).
To simulate the dissociation between the eye gaze (visual attention) and the
intended target (mental attention) which could occur in patients with hampered eye
motor control, healthy participants are cued to operate the BCI in specific
\ac{vsa} conditions.


\subsubsection{Patient case study}
\todo{mention multi-center patient study case studies in cooperation
After data collection from healthy participants and decoder development, we
will carry out a patient study using a similar interface.
The goal of the patient study is to evaluate the influence of eye motor
deficits in patient populations on existing state-of-the-art decoders and on
our proposed decoding methods.
}
\todo{update with current results}

\todo{refer to chapters}
